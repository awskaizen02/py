{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40b5fe79-c82f-463f-aa74-da85dd002e2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-----------+-----+\n|Customer Name|       Category|Subcategory|Sales|\n+-------------+---------------+-----------+-----+\n| Tom Prescott|      Furniture|     Chairs| 4000|\n| Tom Prescott|      Furniture|     Tables| 5000|\n| Tom Prescott|      Furniture|      Sofas| 6000|\n| Tom Prescott|      Furniture|       Beds| 7000|\n| Tom Prescott|Office Supplies|    Binders| 8000|\n| Tom Prescott|Office Supplies|   Supplies| 4000|\n| Tom Prescott|Office Supplies|    Storage| 4000|\n| Tom Prescott|Office Supplies|  Fasteners| 6000|\n| Tom Prescott|     Technology|   Machines|20000|\n|     John Mur|     Technology|    Copiers|20000|\n|     John Mur|     Technology|   Printers|20000|\n|     John Mur|     Technology|   Scanners|15000|\n|     John Mur|     Technology| Projectors|10000|\n+-------------+---------------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "sampleDate = [\n",
    "    ('Tom Prescott', 'Furniture', 'Chairs', 4000),\n",
    "    ('Tom Prescott', 'Furniture', 'Tables', 5000),\n",
    "    ('Tom Prescott', 'Furniture', 'Sofas', 6000),\n",
    "    ('Tom Prescott', 'Furniture', 'Beds', 7000),\n",
    "    ('Tom Prescott', 'Office Supplies', 'Binders', 8000),\n",
    "    ('Tom Prescott', 'Office Supplies', 'Supplies', 4000),\n",
    "    ('Tom Prescott', 'Office Supplies', 'Storage', 4000),\n",
    "    ('Tom Prescott', 'Office Supplies', 'Fasteners', 6000),\n",
    "    ('Tom Prescott', 'Technology', 'Machines', 20000),\n",
    "    ('John Mur','Technology', 'Copiers', 20000),\n",
    "      ('John Mur','Technology', 'Printers', 20000),\n",
    "      ('John Mur','Technology', 'Scanners', 15000),\n",
    "       ('John Mur','Technology', 'Projectors', 10000)\n",
    "]\n",
    "# column names for dataframe\n",
    "columns = ['Customer Name', 'Category', 'Subcategory', 'Sales']\n",
    "# create dataframe\n",
    "df = spark.createDataFrame(sampleDate, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36c06051-7a91-498a-89e0-ffb3cf6fd8af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * # this has to used when you are trying to use transformation functions\n",
    "from pyspark.sql.types import * # this has to be used when you are trying to assign datatypes and struct field and struct type\n",
    "from pyspark.sql.window import * # this has to used when you are working with window functions\n",
    "from pyspark.sql.column import *  # this has to used when you are working with col in dataframe\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67fe6d8f-5de1-4fef-abab-f31666752865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# to create a window function in pyspark, there are two steps.\n",
    "# step1: assign a partition nd order level, either table or specific column partion levels\n",
    "# step2: write any functions based on above partition created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0ca0f2f-79eb-4938-8bd5-5bb4629adc03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "example for  SQL table level : over(order by sales desc)\n",
    "pyspark code for table level: Window.orderBy(col('sales').desc())\n",
    "\n",
    "example for SQL partiti on level: over(partition by category order by sales desc)\n",
    "pyspark code for partition level: Window.partitionBy(\"Category\").orderBy(col('sales').desc())\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9e80120-2f8d-4c31-a4f6-745eb68932ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1061: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-----------+-----+----------+\n|Customer Name|       Category|Subcategory|Sales|row_number|\n+-------------+---------------+-----------+-----+----------+\n| Tom Prescott|     Technology|   Machines|20000|         1|\n|     John Mur|     Technology|    Copiers|20000|         2|\n|     John Mur|     Technology|   Printers|20000|         3|\n|     John Mur|     Technology|   Scanners|15000|         4|\n|     John Mur|     Technology| Projectors|10000|         5|\n| Tom Prescott|Office Supplies|    Binders| 8000|         6|\n| Tom Prescott|      Furniture|       Beds| 7000|         7|\n| Tom Prescott|      Furniture|      Sofas| 6000|         8|\n| Tom Prescott|Office Supplies|  Fasteners| 6000|         9|\n| Tom Prescott|      Furniture|     Tables| 5000|        10|\n| Tom Prescott|      Furniture|     Chairs| 4000|        11|\n| Tom Prescott|Office Supplies|   Supplies| 4000|        12|\n| Tom Prescott|Office Supplies|    Storage| 4000|        13|\n+-------------+---------------+-----------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# create a row_number() for above table based on sales descending\n",
    "\n",
    "#step-1: Create a window partition at table level\n",
    "\n",
    "table_partition = Window.orderBy(col('sales').desc())\n",
    "\n",
    "#step-2: cREATE A  row_number() function \n",
    "df2 = df.withColumn('row_number', row_number().over(table_partition)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd96ba6a-c6d9-4264-8232-ce80a8e6aef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-----------+-----+----------+----+----------+\n|Customer Name|       Category|Subcategory|Sales|row_number|rank|dense_rank|\n+-------------+---------------+-----------+-----+----------+----+----------+\n| Tom Prescott|      Furniture|       Beds| 7000|         1|   1|         1|\n| Tom Prescott|      Furniture|      Sofas| 6000|         2|   2|         2|\n| Tom Prescott|      Furniture|     Tables| 5000|         3|   3|         3|\n| Tom Prescott|      Furniture|     Chairs| 4000|         4|   4|         4|\n| Tom Prescott|Office Supplies|    Binders| 8000|         1|   1|         1|\n| Tom Prescott|Office Supplies|  Fasteners| 6000|         2|   2|         2|\n| Tom Prescott|Office Supplies|   Supplies| 4000|         3|   3|         3|\n| Tom Prescott|Office Supplies|    Storage| 4000|         4|   3|         3|\n| Tom Prescott|     Technology|   Machines|20000|         1|   1|         1|\n|     John Mur|     Technology|    Copiers|20000|         2|   1|         1|\n|     John Mur|     Technology|   Printers|20000|         3|   1|         1|\n|     John Mur|     Technology|   Scanners|15000|         4|   4|         2|\n|     John Mur|     Technology| Projectors|10000|         5|   5|         3|\n+-------------+---------------+-----------+-----+----------+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create a row_number() for above table based on category partition and sales descending\n",
    "\n",
    "#step-1: Create a window partition at table level\n",
    "window_cat = Window.partitionBy(\"Category\").orderBy(col('sales').desc())\n",
    "\n",
    "#step-2: cREATE A  row_number() function \n",
    "df2 = df.withColumn('row_number', row_number().over(window_cat))\\\n",
    "    .withColumn('rank', rank().over(window_cat))\\\n",
    "        .withColumn('dense_rank', dense_rank().over(window_cat))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3586e45a-f45c-4037-9f6c-6ee3b72155ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lag : get the previous row value into next row\n",
    "# syntax: lead (\"COLNAME\",<NO_OF_NEXTCOLUMN>).over(<PARTATION LEVEL>)\n",
    "# Lead: get the last row value into current row\n",
    "# syntax: lag (\"COLNAME\",<NO_OF_LASTCOLUMN>).over(<PARTATION LEVEL>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6af8b076-98c6-4ca7-9f62-f7925b322e7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1061: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-----------+-----+-----+-----+\n|Customer Name|       Category|Subcategory|Sales| lag1| lag2|\n+-------------+---------------+-----------+-----+-----+-----+\n| Tom Prescott|     Technology|   Machines|20000| NULL| NULL|\n|     John Mur|     Technology|    Copiers|20000|20000| NULL|\n|     John Mur|     Technology|   Printers|20000|20000|20000|\n|     John Mur|     Technology|   Scanners|15000|20000|20000|\n|     John Mur|     Technology| Projectors|10000|15000|20000|\n| Tom Prescott|Office Supplies|    Binders| 8000|10000|15000|\n| Tom Prescott|      Furniture|       Beds| 7000| 8000|10000|\n| Tom Prescott|      Furniture|      Sofas| 6000| 7000| 8000|\n| Tom Prescott|Office Supplies|  Fasteners| 6000| 6000| 7000|\n| Tom Prescott|      Furniture|     Tables| 5000| 6000| 6000|\n| Tom Prescott|      Furniture|     Chairs| 4000| 5000| 6000|\n| Tom Prescott|Office Supplies|   Supplies| 4000| 4000| 5000|\n| Tom Prescott|Office Supplies|    Storage| 4000| 4000| 4000|\n+-------------+---------------+-----------+-----+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Get THE PREVIOUS ROW SALES INTO CURRENT ROW\n",
    "\n",
    "#STEP 1: CREATE A WINDOW PARTITION AT TABLE LEVEL\n",
    "table_partition = Window.orderBy(col('sales').desc())\n",
    "\n",
    "#step-2: cREATE A  lag Function\n",
    "df2 = df.withColumn(\"lag1\",lag(\"sales\",1).over(table_partition))\\\n",
    "    .withColumn(\"lag2\",lag(\"sales\",2).over(table_partition))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9d04bad-545b-4971-8ffe-0bdd18a79f97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1061: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-----------+-----+-----+-----+\n|Customer Name|       Category|Subcategory|Sales|lead1|lead2|\n+-------------+---------------+-----------+-----+-----+-----+\n| Tom Prescott|     Technology|   Machines|20000|20000|20000|\n|     John Mur|     Technology|    Copiers|20000|20000|15000|\n|     John Mur|     Technology|   Printers|20000|15000|10000|\n|     John Mur|     Technology|   Scanners|15000|10000| 8000|\n|     John Mur|     Technology| Projectors|10000| 8000| 7000|\n| Tom Prescott|Office Supplies|    Binders| 8000| 7000| 6000|\n| Tom Prescott|      Furniture|       Beds| 7000| 6000| 6000|\n| Tom Prescott|      Furniture|      Sofas| 6000| 6000| 5000|\n| Tom Prescott|Office Supplies|  Fasteners| 6000| 5000| 4000|\n| Tom Prescott|      Furniture|     Tables| 5000| 4000| 4000|\n| Tom Prescott|      Furniture|     Chairs| 4000| 4000| 4000|\n| Tom Prescott|Office Supplies|   Supplies| 4000| 4000| NULL|\n| Tom Prescott|Office Supplies|    Storage| 4000| NULL| NULL|\n+-------------+---------------+-----------+-----+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# get the last row value into current row\n",
    "#step 1: create a window partition at table level\n",
    "table_partition = Window.orderBy(col('sales').desc())\n",
    "#step-2: create a lead function\n",
    "df2 = df.withColumn(\"lead1\",lead(\"sales\",1).over(table_partition))\\\n",
    "    .withColumn(\"lead2\",lead(\"sales\",2).over(table_partition))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6d28e7c-2634-4d2e-9bca-8ca325b5635a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+direct_to_type_checking": {}
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8843907571725001>, line 6\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#step 1: create a window partition at table level\u001B[39;00m\n",
       "\u001B[1;32m      5\u001B[0m partition_cat \u001B[38;5;241m=\u001B[39m Window\u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCategory\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39morderBy(col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msales\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mdesc())\n",
       "\u001B[0;32m----> 6\u001B[0m df2 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mR_N\u001B[39m\u001B[38;5;124m'\u001B[39m, row_number()\u001B[38;5;241m.\u001B[39mover(partition_cat))\\\n",
       "\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mR_N\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m)\n",
       "\u001B[1;32m      8\u001B[0m df2\u001B[38;5;241m.\u001B[39mshow()\n",
       "\u001B[1;32m     10\u001B[0m df3 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mR_N\u001B[39m\u001B[38;5;124m'\u001B[39m, row_number()\u001B[38;5;241m.\u001B[39mover(partition_cat))\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'withColumn'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AttributeError",
        "evalue": "'NoneType' object has no attribute 'withColumn'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'NoneType' object has no attribute 'withColumn'"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
        "File \u001B[0;32m<command-8843907571725001>, line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#step 1: create a window partition at table level\u001B[39;00m\n\u001B[1;32m      5\u001B[0m partition_cat \u001B[38;5;241m=\u001B[39m Window\u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCategory\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39morderBy(col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msales\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mdesc())\n\u001B[0;32m----> 6\u001B[0m df2 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mR_N\u001B[39m\u001B[38;5;124m'\u001B[39m, row_number()\u001B[38;5;241m.\u001B[39mover(partition_cat))\\\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mR_N\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m      8\u001B[0m df2\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m     10\u001B[0m df3 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mR_N\u001B[39m\u001B[38;5;124m'\u001B[39m, row_number()\u001B[38;5;241m.\u001B[39mover(partition_cat))\n",
        "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'withColumn'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get top 2 Sales from each category\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "#step 1: create a window partition at table level\n",
    "partition_cat = Window.partitionBy(\"Category\").orderBy(col('sales').desc())\n",
    "df2 = df.withColumn('R_N', row_number().over(partition_cat))\\\n",
    "    .filter(col('R_N') <= 2)\n",
    "df2.show()\n",
    "\n",
    "df3 = df.withColumn('R_N', row_number().over(partition_cat))\n",
    "df3=df3.filter(\"R_N <= 2\")\n",
    "df3.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "357deb8b-60e0-4dc9-a8c9-07bf85d8b9b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+--------+\n|CustoemrID|CustomerName|      date|Category|\n+----------+------------+----------+--------+\n|         1|       Alice|2025-01-01|       A|\n|         1|       Alice|2025-09-02|       B|\n|         1|       Alice|2025-04-03|       C|\n|         1|       Alice|2024-01-04|       D|\n|         2|         Bob|2024-02-01|       E|\n|         2|         Bob|2024-02-02|       F|\n|         2|         Bob|2024-02-03|       G|\n|         2|         Bob|2024-02-04|       H|\n|         3|     Charlie|2024-03-01|       I|\n|         3|     Charlie|2024-03-02|       J|\n|         3|     Charlie|2024-03-03|       K|\n|         3|     Charlie|2024-03-04|       L|\n|         4|       David|2024-04-01|       M|\n|         4|       David|2024-04-02|       N|\n|         4|       David|2024-04-03|       O|\n|         4|       David|2024-04-04|       P|\n|         5|        Emma|2024-05-01|       Q|\n|         5|        Emma|2024-05-02|       R|\n|         5|        Emma|2024-05-03|       S|\n|         5|        Emma|2024-05-04|       T|\n+----------+------------+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"DuplicatesRemove\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data =[\n",
    "    (1, 'Alice', '2025-01-01', 'A'), (1, 'Alice', '2025-09-02', 'B'), (1, 'Alice', '2025-04-03', 'C'), (1, 'Alice', '2024-01-04', 'D'),\n",
    "    (2, 'Bob', '2024-02-01', 'E'), (2, 'Bob', '2024-02-02', 'F'), (2, 'Bob', '2024-02-03', 'G'), (2, 'Bob', '2024-02-04', 'H'),\n",
    "    (3, 'Charlie', '2024-03-01', 'I'), (3, 'Charlie', '2024-03-02', 'J'), (3, 'Charlie', '2024-03-03', 'K'), (3, 'Charlie', '2024-03-04', 'L'),\n",
    "    (4, 'David', '2024-04-01', 'M'), (4, 'David', '2024-04-02', 'N'), (4, 'David', '2024-04-03', 'O'), (4, 'David', '2024-04-04', 'P'),\n",
    "    (5, 'Emma', '2024-05-01', 'Q'), (5, 'Emma', '2024-05-02', 'R'), (5, 'Emma', '2024-05-03', 'S'), (5, 'Emma', '2024-05-04', 'T')\n",
    "]\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, ['CustoemrID', 'CustomerName', 'date', 'Category'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a62e0664-bfbe-47e7-953d-d209f45fba51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+--------+\n|CustoemrID|CustomerName|      date|Category|\n+----------+------------+----------+--------+\n|         1|       Alice|2025-01-01|       A|\n|         1|       Alice|2025-09-02|       B|\n|         1|       Alice|2025-04-03|       C|\n|         1|       Alice|2024-01-04|       D|\n|         2|         Bob|2024-02-01|       E|\n|         2|         Bob|2024-02-02|       F|\n|         2|         Bob|2024-02-03|       G|\n|         2|         Bob|2024-02-04|       H|\n|         3|     Charlie|2024-03-01|       I|\n|         3|     Charlie|2024-03-02|       J|\n|         3|     Charlie|2024-03-03|       K|\n|         3|     Charlie|2024-03-04|       L|\n|         4|       David|2024-04-01|       M|\n|         4|       David|2024-04-02|       N|\n|         4|       David|2024-04-03|       O|\n|         4|       David|2024-04-04|       P|\n|         5|        Emma|2024-05-01|       Q|\n|         5|        Emma|2024-05-02|       R|\n|         5|        Emma|2024-05-03|       S|\n|         5|        Emma|2024-05-04|       T|\n+----------+------------+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30d76412-ec63-4fbc-9f0a-a7fba582f87f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+--------+---+\n|CustoemrID|CustomerName|      date|Category|R_N|\n+----------+------------+----------+--------+---+\n|         1|       Alice|2025-09-02|       B|  1|\n|         2|         Bob|2024-02-04|       H|  1|\n|         3|     Charlie|2024-03-04|       L|  1|\n|         4|       David|2024-04-04|       P|  1|\n|         5|        Emma|2024-05-04|       T|  1|\n+----------+------------+----------+--------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Get latest date transaction for each customer\n",
    "#step 1: create a window partition at table level\n",
    "partition_cat = Window.partitionBy(\"CustomerName\").orderBy(col('date').desc())\n",
    "df_lastdate = df.withColumn('R_N', row_number().over(partition_cat))\n",
    "df_lastdate = df_lastdate.filter(\"R_N = 1\")\n",
    "df_lastdate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f174ae42-7600-45fe-9cf8-a22fa57bee21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+--------+\n|CustoemrID|CustomerName|      date|Category|\n+----------+------------+----------+--------+\n|     10000|       Alice|2024-01-01|       A|\n|     10000|       Alice|2024-01-02|       B|\n|      5000|         Bob|2024-02-01|       E|\n|      5000|         Bob|2024-02-02|       F|\n|      5000|         Bob|2024-02-03|       G|\n|      5000|         Bob|2024-02-04|       H|\n|      3000|     Charlie|2024-03-01|       I|\n+----------+------------+----------+--------+\n\n+-----------+\n|SalesAmount|\n+-----------+\n|      10000|\n|      10000|\n|       5000|\n|       5000|\n|       5000|\n|       5000|\n|       3000|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"listdataframe\").getOrCreate()\n",
    "\n",
    "# Sample list\n",
    "list1 = [\n",
    "        (10000, 'Alice', '2024-01-01', 'A'),\n",
    "    (10000, 'Alice', '2024-01-02', 'B'),\n",
    "    (5000, 'Bob', '2024-02-01', 'E'),\n",
    "    (5000, 'Bob', '2024-02-02', 'F'),\n",
    "    (5000, 'Bob', '2024-02-03', 'G'),\n",
    "    (5000, 'Bob', '2024-02-04', 'H'),\n",
    "    (3000, 'Charlie', '2024-03-01', 'I')\n",
    "\n",
    "]\n",
    "\n",
    "# create DataFrame\n",
    "df = spark.createDataFrame(list1, ['CustoemrID', 'CustomerName', 'date', 'Category'])\n",
    "df.show()\n",
    "df = df.select(df.CustoemrID.alias(\"SalesAmount\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fda254e9-8659-4547-90dc-e8094a648e19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|SalesAmount|\n+-----------+\n|      10000|\n|      10000|\n|       5000|\n|       5000|\n|       5000|\n|       5000|\n|       3000|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c36e851-483f-42b1-b79b-9945a1e05de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1061: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----------+----------+\n|SalesAmount|rank|dense_rank|row_number|\n+-----------+----+----------+----------+\n|      10000|   1|         1|         1|\n|      10000|   1|         1|         2|\n|       5000|   3|         2|         3|\n|       5000|   3|         2|         4|\n|       5000|   3|         2|         5|\n|       5000|   3|         2|         6|\n|       3000|   7|         3|         7|\n+-----------+----+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, rank, dense_rank, row_number\n",
    "\n",
    "part_rank = Window.orderBy(col(\"SalesAmount\").desc())\n",
    "df2 = df.withColumn(\"rank\", rank().over(part_rank))\\\n",
    "      .withColumn(\"dense_rank\", dense_rank().over(part_rank))\\\n",
    "      .withColumn(\"row_number\", row_number().over(part_rank))\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b508812-a961-49b3-9939-31cce8a80c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Window Functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}